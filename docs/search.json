[
  {
    "objectID": "results.html",
    "href": "results.html",
    "title": "Results",
    "section": "",
    "text": "The empirical performance of all agents in Bellman’s Bakery is summarized below. The results should be read through the lens of reward shaping and capacity constraints: throughput is limited by both service speed and oven space, while the shaped reward penalizes waiting, abandonment, and excess inventory. Within this structure, agents learn to optimize the reward they are given—not necessarily profit. The Newsvendor heuristic achieves relatively high profit by baking aggressively and accepting waste as a cost of doing business. PPO, by contrast, learns a conservative, low-waste strategy that under-produces and sacrifices margin. QRDQN explores more broadly and improves profit in some runs, but its decisions still reflect the incentives baked into the reward function. Pricing focused solely on profit increases margins slightly but leaves the production strategy mostly unchanged."
  },
  {
    "objectID": "results.html#full-horizon-evaluation",
    "href": "results.html#full-horizon-evaluation",
    "title": "Results",
    "section": "Full-Horizon Evaluation",
    "text": "Full-Horizon Evaluation\nMetrics aggregate 10 independent seeds; each seed is evaluated for 20 full days (240 ticks/day) under non‑stationary demand. Daily results are averaged per seed and then across seeds to approximate long‑run behavior.\nMetrics reported: - Net Profit ($) - Abandoned (% of customers) - Avg Wait (seconds) - Waste (% of items baked that are discarded)\n\n\n\n\n\n\n\n\n\n\nPolicy\nNet Profit\nAbandoned\nAvg Wait (s)\nWaste\n\n\n\n\nNewsvendor (baseline)\n95.22\n1.5%\n12.7\n41.1%\n\n\nPPO (best)\n35.28\n74.7%\n49.1\n14.2%\n\n\nPPO+Price (best)\n23.36\n81.3%\n52.1\n37.7%\n\n\nPPO+Par\n-20.47\n91.1%\n56.0\n82.8%\n\n\nPPO+Price (profit-only)\n25.79\n83.5%\n53.2\n44.5%\n\n\nQRDQN (best)\n41.32\n21.2%\n22.7\n44.9%\n\n\n\n\nPolicy Key\n\n\n\n\n\n\n\n\n\nPolicy\nFamily\nMechanism / Extras\nOptimized objective\n\n\n\n\nNewsvendor (baseline)\nHeuristic\nTarget-based baking toward expected demand; accepts waste\nExpected profit via service‑level (quantile) target\n\n\nPPO (best)\nRL (on‑policy)\nMaskablePPO; serve-first + capacity masking\nShaped reward\n\n\nPPO+Price (best)\nRL + Bandit\nMasked PPO + Thompson‑sampling daily price\nComposite (reward‑aligned)\n\n\nPPO+Par\nHybrid (RL + rule)\nMasked PPO + fixed par‑level inventory caps\nShaped reward\n\n\nPPO+Price (profit‑only)\nRL + Bandit\nMasked PPO + Thompson‑sampling daily price\nBandit optimizes profit only\n\n\nQRDQN (best)\nRL (off‑policy, distributional)\nNo action masking; replay + quantile value estimates\nShaped reward\n\n\n\nNote: “best” = checkpoint with highest evaluation net profit (averaged across seeds) among runs.\n\n\nInterpretation\nNewsvendor dominates by baking aggressively, tolerating waste, and sustaining throughput. In contrast, PPO gravitates toward an under-producing regime, shaped by an objective that penalizes waiting, abandonment, and leftover inventory. The resulting behavior achieves very low waste, but at a cost: chronic stockouts, high abandonment, and long waits. In short, PPO is optimal for the reward it sees—not for economic profit.\nBandit variants follow a similar pattern. Composite-metric pricing amplifies the agent’s conservative stance, while profit-only pricing pushes margins slightly higher without meaningfully shifting production behavior. PPO+Par performs worst overall, as the par mechanism locks the agent into persistent underproduction with no viable path to adjust.\nTakeaway: The RL agents are not failing. They are learning exactly what they were told to optimize—and that turns out to be the wrong thing."
  },
  {
    "objectID": "results.html#weekly-aggregation-a-check-for-robustness",
    "href": "results.html#weekly-aggregation-a-check-for-robustness",
    "title": "Results",
    "section": "Weekly Aggregation: A check for robustness",
    "text": "Weekly Aggregation: A check for robustness\nTo test robustness to non‑stationarity, results are aggregated over 5 consecutive days per seed (20 seeds total), mimicking week‑level operations. Weekly values are per‑seed averages over those 5 days, then averaged across seeds. Variance increases, but rank ordering and qualitative behavior remain unchanged.\n\n\n\n\n\n\n\n\n\n\nPolicy\nNet Profit\nAbandoned\nAvg Wait (s)\nWaste\n\n\n\n\nNewsvendor weekly\n25.58\n0.4%\n11.5\n40.3%\n\n\nPPO weekly\n20.17\n79.8%\n50.6\n52.6%\n\n\nPPO+Price weekly\n22.68\n81.7%\n52.0\n38.6%\n\n\nPPO+Par weekly\n-20.74\n90.9%\n55.7\n82.8%\n\n\nPPO+Price (profit-only) weekly\n26.36\n83.4%\n52.8\n44.1%\n\n\nQRDQN weekly\n29.04\n21.6%\n22.9\n46.9%\n\n\n\nRankings persist across seeds and week windows, and paired 95% CIs exclude zero, indicating the gaps reflect the reward/capacity structure rather than random variation."
  },
  {
    "objectID": "results.html#statistical-analysis-per-seed",
    "href": "results.html#statistical-analysis-per-seed",
    "title": "Results",
    "section": "Statistical analysis (per-seed)",
    "text": "Statistical analysis (per-seed)\nPer-seed mean profit and 95% confidence intervals are reported. Unless noted otherwise, 95% CIs are normal CIs computed over seed means. Paired differences versus Newsvendor use bootstrap 95% CIs on per‑seed mean differences.\n\n\n\nPolicy\nSeeds (n)\nMean profit\n95% CI (±)\n\n\n\n\nNewsvendor\n10\n103.21\n3.95\n\n\nPPO\n10\n35.28\n1.07\n\n\nQRDQN\n10\n41.32\n11.76\n\n\n\nPaired differences vs Newsvendor (per-seed):\n\nPPO − Newsvendor: −67.94 (95% CI [−72.11, −63.68], n=10)\nQRDQN − Newsvendor: −61.89 (95% CI [−71.45, −51.60], n=10)\n\nThese intervals confirm the large gaps observed in the tables and are not explained by sampling noise."
  },
  {
    "objectID": "results.html#training-dynamics",
    "href": "results.html#training-dynamics",
    "title": "Results",
    "section": "Training Dynamics",
    "text": "Training Dynamics\nTraining stability is assessed via rollout and evaluation reward curves. Curves are smoothed rolling means; higher values indicate better reward performance.\n\n\nAcross methods, there is no divergence or collapse; learning curves are smooth, and observed behavior follows from reward alignment rather than optimization failure."
  },
  {
    "objectID": "results.html#demonstrations-of-trained-policies",
    "href": "results.html#demonstrations-of-trained-policies",
    "title": "Results",
    "section": "Demonstrations of Trained Policies",
    "text": "Demonstrations of Trained Policies\n\nPPO Policy Demo\n\n\n\nQRDQN Policy Demo"
  },
  {
    "objectID": "results.html#findings",
    "href": "results.html#findings",
    "title": "Results",
    "section": "Findings",
    "text": "Findings\n\nReward alignment drives learned behavior: PPO follows the incentives embedded in the shaped reward. Because the reward penalizes waiting, abandonment, and leftover inventory, the agent converges to a conservative production policy. Under-production is not a failure mode; it is the logical outcome of optimizing a reward that treats waste as worse than missed sales.\nHeuristic baselines outperform deep RL under misaligned objectives: Newsvendor succeeds because it encodes domain knowledge that the RL agents are never given: in this system, throughput matters more than waste. Its structural prior—bake aggressively and accept inventory losses—matches the true profit objective, while PPO and QRDQN optimize a proxy objective with different priorities.\nPricing adjustments offer limited gains without objective alignment: Bandit-based pricing lifts profit only when updates use profit directly. Even then, pricing cannot fix PPO’s core behavioral bias toward minimal production. The bandit can shift margins but cannot change the underlying production strategy learned from the shaped reward.\nTraining stability is not the bottleneck: Learning curves are smooth across PPO, PPO+Price, and QRDQN. There is no evidence of instability, divergence, or collapse. The gap between heuristics and RL arises from what the agents are asked to optimize, not from failures in optimization.\n\nTaken together, these findings show that beating strong operational heuristics requires either a reward aligned with business KPIs, RL methods that encode queue and capacity structure explicitly, or hybrid approaches that integrate domain priors. Algorithmic sophistication alone cannot overcome a reward function that penalizes the very behaviors needed for profit."
  },
  {
    "objectID": "methods.html",
    "href": "methods.html",
    "title": "Methods",
    "section": "",
    "text": "This section describes the learning algorithms, pricing bandit, heuristic baselines, and experimental protocols used to evaluate control policies in Bellman’s Bakery. Two reinforcement learning (RL) agents—Proximal Policy Optimization (PPO) and Quantile Regression DQN (QRDQN), are compared against bandit-augmented PPO and established heuristics. All approaches interact with the same simulator described in Environment."
  },
  {
    "objectID": "methods.html#environment-summary-abridged",
    "href": "methods.html#environment-summary-abridged",
    "title": "Methods",
    "section": "Environment summary (abridged)",
    "text": "Environment summary (abridged)\nEach episode spans 240 ticks (10 seconds each) and includes two ovens (capacity 4 units each), a queue capped at 12 customers, and a limit of three customers served per tick. Arrivals follow a nonhomogeneous Poisson process with morning and midday peaks. Demand is nonstationary, exhibiting ±10% daily drift and ±10% weekly fluctuations.\nThe reward is a shaped, multi-objective signal:\n- revenue and a +0.1 serve bonus,\n- a wait penalty of 0.01 per tick per queued customer,\n- a 0.5 abandonment penalty,\n- a 0.1 balk penalty, and\n- a terminal leftover-cost penalty.\nDaily price multipliers scale revenue. Demand elasticity is minimal, so pricing primarily adjusts revenue rather than arrival intensity."
  },
  {
    "objectID": "methods.html#learning-agents",
    "href": "methods.html#learning-agents",
    "title": "Methods",
    "section": "Learning agents",
    "text": "Learning agents\n\nProximal Policy Optimization (PPO)\nPPO is implemented with MaskablePPO (SB3-Contrib) to ensure feasibility in the 11-action discrete space. Invalid actions are masked by assigning logits of \\(-\\infty\\). In addition, when any serve action is available, all bake actions are masked to enforce service priority.\n\nArchitecture and training setup\n\nPolicy network: two-layer MLP (256–256), Tanh activations.\n\nValue network: shares backbone features with the policy.\n\nVectorized training: 8 parallel environments.\n\n\n\nHyperparameters\n\nn_steps = 2048\n\nbatch_size = 256\n\ngamma = 0.995\n\ngae_lambda = 0.95\n\nclip_range = 0.2\n\nent_coef = 0.01\n\nlearning_rate = 3e-4\n\nTraining horizon: \\(10^6\\) environment steps (preceded by a 300k warm-up run).\n\n\n\nEnvironment-specific guardrails\n\nBake actions masked when any serve action is feasible.\n\nOven-capacity masking for bake actions.\n\nPer-tick cap: serve_per_tick = 3.\n\nPPO maximizes the shaped reward; it is not directly trained to maximize economic profit.\n\n\n\nQuantile Regression DQN (QRDQN)\nQRDQN estimates full return distributions by learning quantile values. In SB3, QRDQN does not natively support action masking; when an invalid action is selected, the environment applies a small penalty and treats the step as idle. Exploration follows ε-greedy decay.\n\nArchitecture and training setup\n\nMLP architecture: 256–256, Tanh activation.\n\nNumber of quantiles: 51.\n\nReplay buffer: size \\(10^6\\), uniform sampling.\n\nExploration: ε-greedy (1.0 → 0.05 linear decay).\n\nTarget network: Polyak averaging with \\(\\tau = 0.005\\).\n\nOptimizer: Adam, learning rate \\(3\\times 10^{-4}\\).\n\n\n\nHyperparameters\n\nbatch_size = 256\n\ngamma = 0.995\n\nlearning_starts = 50_000\n\ntrain_freq = 4, gradient_steps = 1\n\ntarget_update_interval = 1\n\nTraining horizon: \\(10^6\\) steps (matched to PPO)."
  },
  {
    "objectID": "methods.html#bandit-layer-daily-pricing",
    "href": "methods.html#bandit-layer-daily-pricing",
    "title": "Methods",
    "section": "Bandit layer: daily pricing",
    "text": "Bandit layer: daily pricing\nA Thompson-sampling bandit selects a single daily price multiplier, yielding a two-time-scale design: the bandit chooses the global price, and the RL controller operates within the day.\n\nPrice arms\n\nStandard grid: {0.9, 1.0, 1.1}\n\nProfit-oriented grid: {0.7, 0.85, 1.0, 1.15, 1.3}\n\n\n\nUpdate metrics\nTwo evaluation signals are considered:\n\nComposite metric (aligned with RL reward shaping):\n\\[\n\\text{metric}\n  = \\text{profit}\n- 0.02 \\cdot \\text{wait\\_ticks}\n- 5 \\cdot \\text{abandoned}\n- 2 \\cdot \\text{balked}.   \n  \\]\nNet-profit-only metric:\n\\[\n\\text{metric} = \\text{net profit}.\n\\]\n\nThe profit-only metric tests whether the bandit can push PPO toward more revenue-oriented behavior."
  },
  {
    "objectID": "methods.html#heuristic-baselines",
    "href": "methods.html#heuristic-baselines",
    "title": "Methods",
    "section": "Heuristic baselines",
    "text": "Heuristic baselines\n\nBake-to-Par\nMaintains fixed per-item inventory targets. The agent bakes the largest deficit first and serves the earliest matching customer.\n\n\nGreedy Queue\nCounts pending requests in the queue and bakes the most requested item; defaults to long-run demand proportions when no item is dominant.\n\n\nNewsvendor (strongest baseline)\nComputes per-item production targets from expected total demand and bakes toward the largest remaining target gap. This heuristic performs strongly in settings where demand is predictable and waste costs are absorbed by high throughput."
  },
  {
    "objectID": "methods.html#training-protocol",
    "href": "methods.html#training-protocol",
    "title": "Methods",
    "section": "Training protocol",
    "text": "Training protocol\nAll learning agents use identical simulator configurations. Training employs 8 vectorized environments and Adam optimization. Models are checkpointed periodically. Reported results include:\n\nthe final checkpoint, and\n\nthe best checkpoint, selected using evaluation net profit averaged over seeds.\n\nRandom seed handling follows SB3 conventions for PPO and QRDQN to ensure reproducibility."
  },
  {
    "objectID": "methods.html#evaluation-protocol",
    "href": "methods.html#evaluation-protocol",
    "title": "Methods",
    "section": "Evaluation protocol",
    "text": "Evaluation protocol\nTwo evaluation regimes are used:\n\nFull horizon: 10 seeds × 20 days; results reported as seed-level means.\n\nWeekly stability: 20 seeds × 5 days; emphasizes variance across seeds.\n\nKey performance indicators (KPIs):\n\nnet profit,\n\nabandonment rate,\n\nmean wait (seconds),\n\nwaste rate,\n\nservice rate.\n\nRL-internal performance is monitored through TensorBoard rollout and evaluation reward curves."
  },
  {
    "objectID": "methods.html#reward-specification-reference",
    "href": "methods.html#reward-specification-reference",
    "title": "Methods",
    "section": "Reward specification (reference)",
    "text": "Reward specification (reference)\n\n\n\nComponent\nMeaning\nWeight\nSign\n\n\n\n\nProfit\nrevenue − cost\n+1.0\npositive\n\n\nWait penalty\nper tick of waiting\n−0.01\nnegative\n\n\nAbandon penalty\nexpired patience\n−0.5\nnegative\n\n\nBalk penalty\nqueue full\n−0.1\nnegative\n\n\nLeftover cost\nunsold items\n−1.0\nnegative\n\n\nServe bonus\neach service\n+0.1\npositive\n\n\n\nFormal definition: \\[\nR_t\n:= \\mathrm{profit}_t\n- \\lambda_w\\,\\mathrm{wait\\_ticks}_t\n- \\lambda_a\\,\\mathrm{abandon}_t\n- \\lambda_b\\,\\mathrm{balk}_t\n- \\lambda_L\\,\\mathrm{leftover}_t\n+ \\lambda_s\\,\\mathrm{served}_t,\n\\]\nwith\n\\(\\lambda_w = 0.01\\),\n\\(\\lambda_a = 0.5\\),\n\\(\\lambda_b = 0.1\\),\n\\(\\lambda_L = 1.0\\),\n\\(\\lambda_s = 0.1\\).\nNote: No idle‑penalty term is included (\\(\\lambda_{\\text{idle}} = 0\\)). An idle penalty was explored in early iterations but was removed in the final experiments; the environment applies no per‑tick penalty for idle actions.\n\nExpected behavioral differences\n\nPPO tends to adopt conservative, waste-minimizing strategies because penalties accumulate over time.\n\nQRDQN, with replay and distributional targets, may explore more aggressive bake/serve trade-offs even under the same reward."
  },
  {
    "objectID": "methods.html#implementation-details-and-reproducibility",
    "href": "methods.html#implementation-details-and-reproducibility",
    "title": "Methods",
    "section": "Implementation details and reproducibility",
    "text": "Implementation details and reproducibility\n\nFrameworks: Stable-Baselines3 2.3.0 and SB3-Contrib 2.3.0.\n\nCore stack pinned: NumPy 1.26.4, Pandas 2.1.4, PyArrow 14.0.2.\n\nTraining and evaluation scripts, seed configurations, and plotting utilities are included in project repositories.\n\nTensorBoard logs are exported for inclusion in the Results section."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reward Misalignment in Deep Reinforcement Learning: Evidence from a Bakery Production Environment",
    "section": "",
    "text": "This project examines reward–behavior alignment in a capacity‑constrained production setting. A custom “Bellman’s Bakery” environment models a single day with two ovens, stochastic arrivals, patience/abandonment, and five products. PPO and QRDQN are evaluated against heuristics. Under a shaped reward that heavily penalizes leftovers and delay, learning converges to conservative policies that under‑produce to avoid waste. Pricing adjustments provide limited leverage because oven throughput is the binding constraint. Profit‑aligned evaluation yields modest increases in service and profit but does not remove the bottleneck. Findings highlight that reward design and structural constraints dominate performance more than the specific deep RL algorithm."
  },
  {
    "objectID": "index.html#abstract",
    "href": "index.html#abstract",
    "title": "Reward Misalignment in Deep Reinforcement Learning: Evidence from a Bakery Production Environment",
    "section": "",
    "text": "This project examines reward–behavior alignment in a capacity‑constrained production setting. A custom “Bellman’s Bakery” environment models a single day with two ovens, stochastic arrivals, patience/abandonment, and five products. PPO and QRDQN are evaluated against heuristics. Under a shaped reward that heavily penalizes leftovers and delay, learning converges to conservative policies that under‑produce to avoid waste. Pricing adjustments provide limited leverage because oven throughput is the binding constraint. Profit‑aligned evaluation yields modest increases in service and profit but does not remove the bottleneck. Findings highlight that reward design and structural constraints dominate performance more than the specific deep RL algorithm."
  },
  {
    "objectID": "index.html#demo",
    "href": "index.html#demo",
    "title": "Reward Misalignment in Deep Reinforcement Learning: Evidence from a Bakery Production Environment",
    "section": "Demo",
    "text": "Demo\nA short viewer clip of one simulated day (Newsvendor baseline) is embedded below.\n\nYour browser does not support the video tag.\n\n\nClip shows baking/serving decisions, oven progress, queue, and current action.\n\nQuickstart and viewer controls are documented in the project README."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Reward Misalignment in Deep Reinforcement Learning: Evidence from a Bakery Production Environment",
    "section": "Overview",
    "text": "Overview\nBellman’s Bakery is a pastel, “tiny bakery tycoon” environment used to study reward–behavior alignment in deep RL. Each episode is a single day with two ovens, five products, stochastic arrivals, and customer patience/abandonment. The work evaluates PPO and QRDQN alongside simple baselines and price bandits under capacity constraints.\nAt a glance\n\nEnvironment: two-oven production with queueing, bake times, sizes, and yields.\nAlgorithms: PPO (masking) and QRDQN; bandit pricing for daily multipliers.\nBaselines: newsvendor, bake‑to‑par, greedy‑queue.\nKey finding: shaped rewards that over‑penalize leftovers/delay lead to conservative, under‑producing policies; pricing has limited leverage under throughput bottlenecks.\n\nRun locally (viewer):\npython -m scripts.run_viewer --model models/best_ppo.zip\n# or run a baseline without a model:\npython -m scripts.run_viewer --baseline newsvendor"
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion and Future Work",
    "section": "",
    "text": "The results underscore a fundamental principle in reinforcement learning (RL) for operational challenges: an agent’s actions are influenced by the rewards it receives, rather than the criteria by which humans ultimately assess success. The Newsvendor heuristic makes the most money in the Tiny Bakery environment because it uses a strategy that the RL reward doesn’t like: making a lot of food, keeping inventory buffers, and accepting waste as a cost of keeping service levels high.\nPPO, on the other hand, optimizes a composite reward that gives a lot of weight to wait times, abandonment, and leftover inventory. In order to reach that goal, the agent acts exactly as expected: it uses a conservative baking policy that limits exposure to negative reward terms but lowers throughput, service quality, and, in the end, profit. When trained on the same composite reward, the pricing bandit reinforces this pattern. Even the profit-only version can only change behavior a little bit because pricing can’t fix the underlying capacity problems in ovens and queue dynamics.\nThese results are similar to what has been found in recent studies on RL for perishable goods, service systems, and dynamic pricing (Nomura, Liu, and Nishi 2025; van Hezewijk 2024; Genalti 2021). In these studies, agents often look “suboptimal” on business metrics but are perfectly optimal under the shaped reward. They also fit with what reward design theory has said for a long time: if the shaped reward doesn’t keep the best policy under transformation, even well-meaning punishments can lead to behavior that isn’t economically desirable (Ng, Harada, and Russell 1999). In this instance, the reward effectively instructs agents to perceive overproduction as more perilous than lost sales.\nThe experiments do not point out algorithmic failure; instead, they bring up the bigger problem of “objective misalignment,” which is a well-known problem in the RL-for-operations literature and a major focus of safe reinforcement learning research (Garcı́a and Fernández 2015)."
  },
  {
    "objectID": "conclusion.html#future-directions",
    "href": "conclusion.html#future-directions",
    "title": "Conclusion and Future Work",
    "section": "Future Directions",
    "text": "Future Directions\nThere are a few extensions that could help close the alignment gap and make learning in the bakery environment more dynamic. One option is to change the rewards and set limits on the goals. A reward system that is based on profit or a limited RL formulation, like profit maximization with service-level penalties, could change behavior in a way that makes policies more economically sound. Techniques such as Lagrangian relaxation, CVaR-based learning, or multi-objective shaping provide more principled methods for encoding trade-offs among waste, service quality, and inventory risk. These methods are in line with the goals of risk-aware or safe reinforcement learning, which tries to find a balance between expected returns and downside control (Garcı́a and Fernández 2015).\nAnother direction is training the curriculum and the environment. It might be easier for the agent to get out of conservative local optima if they start with stationary demand and then move on to non-stationarity. This is similar to successful methods used in earlier research on pricing and inventory, where agents learn to generalize better when they are trained in steps.\nIt may also be very important to come up with “capacity-aware strategies.” Short-horizon lookahead, bake-batch optimization, or different oven configurations are some of the new tools that the agent could use to deal with starvation regimes. In the same way, allowing limited restocking or soft limits on par levels could make the policy space bigger without making things less clear.\nAnother area of research could be better baselines and hybrid methods. More expressive par heuristics, contextual bandits for pricing, or RL–heuristic hybrids could better combine what we already know about a domain with what we learn. Newsvendor doesn’t work because it’s flexible; it works because its structural assumptions fit the problem well. Incorporating those assumptions into learning frameworks could produce more resilient, adaptable solutions.\nFinally, the role of alternative RL algorithms deserves closer examination. Value-based or distributional methods, like QRDQN or variants of soft actor-critic (SAC), may interact differently with the same reward function—especially when long-tailed return distributions matter. As QRDQN experiments complete, a comparative analysis will help disentangle whether the conservative bias is due to the algorithm itself or to the underlying objective. Algorithms that clearly show uncertainty may be better at balancing risk and throughput in the long run.\nIn summary, these directions make it clear that the main problem is not adjusting PPO or adding more layers to the architecture. It is creating reward signals that convey operational objectives, such as profitability and service quality, in a manner that the agent can effectively optimize. The experiments presented herein provide a definitive diagnosis of the misalignment and a strategic framework for developing reinforcement learning systems that more accurately embody real-world business priorities."
  },
  {
    "objectID": "analysis.html",
    "href": "analysis.html",
    "title": "Analysis & Discussion",
    "section": "",
    "text": "The empirical findings from Bellman’s Bakery demonstrate a distinct and consistent pattern: the reinforcement learning (RL) agents perform exactly as trained, optimizing their shaped reward functions with precision. That being said, this behavior is often not what would be considered economically rational when looking at business metrics like net profit. The following is a summary of what was observed in Proximal Policy Optimization (PPO) behavior, how pricing affects it, and how it relates to other work."
  },
  {
    "objectID": "analysis.html#behavior-of-ppo-in-a-capacity-constrained-bakery",
    "href": "analysis.html#behavior-of-ppo-in-a-capacity-constrained-bakery",
    "title": "Analysis & Discussion",
    "section": "Behavior of PPO in a Capacity-Constrained Bakery",
    "text": "Behavior of PPO in a Capacity-Constrained Bakery\nAcross all PPO variants, the agent consistently converges to a policy that minimizes penalties for waste, abandonment, and wait times by sharply limiting production. This outcome should not be misread as a failure to learn, rather, it is a rational response to the shaped reward, which imposes strong penalties on leftover/wasted inventory and only modest gains for additional service or throughput.\nWhen oven space is limited and stochastic customer arrivals are a factor, making excess production leads to wasted stock. On the flip side, producing less cuts down on losses, stabilizing rewards despite shifting demand. This pushes agents toward keeping output low, cutting both inventory and waste, even if it means more lost sales or longer queues.\nThis behavior effectively represents a risk-averse approximation of the shaped reward, reflecting how agents prioritize safety and penalty avoidance over reward maximization (Garcı́a and Fernández 2015). PPO is not learning a profit-maximizing policy; it is optimizing a composite signal that encodes waste aversion more strongly than revenue generation. This aligns with safe RL formulations where reward signals bias agents toward conservative behavior under uncertainty (Garcı́a and Fernández 2015).\nIn earlier versions of the environment, a small idle penalty was included to discourage the agent from stalling. Ironically, this had the opposite effect: it pushed the agent to avoid acting altogether when uncertain, over-prioritizing idling to sidestep the risk of incurring other penalties. After removing this penalty and implementing serve-first action masking to encourage throughput, idling was substantially reduced—but the broader production strategy stayed the same. This makes clear that the underlying issue was never policy instability or poor convergence but that the agent was behaving rationally under the incentives it was given.\nThis shows a bigger structural problem: when the reward function encodes a distorted set of trade-offs, agents learn policies that don’t work. In this instance, the RL agent learned to avoid waste so strongly that it stopped production, even when there was a lot of demand. This illustrates a well-documented issue in the reward shaping literature: if the shaped reward fails to maintain the optimal policy under transformation, even minor misalignments can result in learned behaviors that significantly deviate from human designer expectations (Ng, Harada, and Russell 1999)."
  },
  {
    "objectID": "analysis.html#misalignment-between-reward-and-business-metrics",
    "href": "analysis.html#misalignment-between-reward-and-business-metrics",
    "title": "Analysis & Discussion",
    "section": "Misalignment Between Reward and Business Metrics",
    "text": "Misalignment Between Reward and Business Metrics\nWhile the true business goal is to maximize net profit while preserving service quality, the shaped reward used during training captures a bundle of operational priorities:\n\nper-tick wait cost\n\nabandonment penalty\n\nbalking penalty\n\nleftover inventory cost\n\na modest serve bonus\n\nThe purpose of this composite objective was to make learning more stable and make operational realism more real. But it unintentionally overweights penalties for waste and delay, which makes the agent think that being cautious is the best way to act. In a bakery like this one, where there is a lot of variation and not much room for error, the best way to run a business is often to “bake ahead of demand” and accept some waste in exchange for throughput. The reward stops this from happening, which leads to learned policies that seem to be bad for profits, even though they are the best for the shaped goal."
  },
  {
    "objectID": "analysis.html#limited-effectiveness-of-pricing-adaptation",
    "href": "analysis.html#limited-effectiveness-of-pricing-adaptation",
    "title": "Analysis & Discussion",
    "section": "Limited Effectiveness of Pricing Adaptation",
    "text": "Limited Effectiveness of Pricing Adaptation\nTwo bandit-based pricing variants were assessed to determine if dynamic price adjustments could influence behavior towards increased profitability. The results indicate that pricing enhances performance solely when price alterations significantly influence demand volume. Service capacity, not demand, is what limits Bellman’s Bakery. Training the pricing bandit with a reward of the same shape strengthens PPO’s conservative tendencies. Changing prices doesn’t fix problems with baking or waiting in line; instead, it subtly reinforces the desire to avoid waste. When the bandit is optimized directly for profit, the expected response is observed: slightly higher margins and slightly more waste, indicating greater tolerance for risk. However, the overall effect remains small because pricing cannot resolve a bottleneck caused by physical capacity. This is consistent with dynamic pricing literature in constrained systems: pricing only works when it can influence the actual point of friction, which in this case is not price elasticity but limited service throughput."
  },
  {
    "objectID": "analysis.html#ablation-par-level-hybrid-behavior",
    "href": "analysis.html#ablation-par-level-hybrid-behavior",
    "title": "Analysis & Discussion",
    "section": "Ablation: Par-Level Hybrid Behavior",
    "text": "Ablation: Par-Level Hybrid Behavior\nThe PPO+Par hybrid agent significantly underperforms relative to both standalone PPO and heuristic baselines. Par-level logic was introduced as an interpretable heuristic, but it imposes a rigid production ceiling that fails to adapt to demand fluctuations. The par mechanism enforces persistent underproduction, locking the agent into a starvation regime from which neither the policy nor the heuristic can recover. While additional tuning might improve this baseline, the deeper issue lies in its inflexibility. In non-stationary environments, policies must adapt dynamically to demand drift. Par logic, as implemented here, lacks that adaptability."
  },
  {
    "objectID": "analysis.html#qrdqn-analysis",
    "href": "analysis.html#qrdqn-analysis",
    "title": "Analysis & Discussion",
    "section": "QRDQN Analysis",
    "text": "QRDQN Analysis\nQuantile Regression DQN (QRDQN) was trained under the same environment and reward shaping as PPO. The distributional critic reduced value‑estimate variance and produced policies that were marginally more willing to bake ahead of demand. The qualitative pattern nevertheless remained: when trained on the shaped reward, QRDQN also converged to a conservative regime that limits production to avoid leftover penalties.\nWhen QRDQN was paired with a profit‑aligned evaluation signal, modest gains were observed relative to the shaped‑reward variant (slightly higher service rates and profit, with slightly more waste). Improvements were incremental rather than transformative, indicating that distributional value estimation alone does not overcome the capacity constraint or reward misalignment. The results support the broader conclusion that reward design and structural bottlenecks—not the choice between policy‑gradient and value‑based methods—dominate performance in this domain."
  },
  {
    "objectID": "analysis.html#connection-to-related-literature",
    "href": "analysis.html#connection-to-related-literature",
    "title": "Analysis & Discussion",
    "section": "Connection to Related Literature",
    "text": "Connection to Related Literature\nThe empirical patterns observed in Bellman’s Bakery align with several strands of work in RL for operations, inventory control, and pricing. Prior research indicates that RL techniques can only approach optimal control when the reward is meticulously aligned with the business objective. Nomura et al. (Nomura, Liu, and Nishi 2025) show that PPO works well in situations where inventory is perishable and the reward structure reflects real profit trade-offs. However, when penalties or shaping terms are more important, PPO converges to behavior that is mathematically optimal under the reward but economically inefficient, which is similar to the conservative underproduction seen here. Similar themes manifest in queueing and service systems. van Hezewijk (van Hezewijk 2024) finds that agents who are trained with penalties for delays or leftovers often choose policies that are too risk-averse, which slows down throughput even when it means less total revenue. PPO’s actions at Bellman’s Bakery, where they try to cut down on waste even if it means running out of stock, are in line with these results.\nWork on dynamic pricing backs up this idea. Genalti (Genalti 2021) says that multi-armed bandit pricing only works when the binding constraint is demand elasticity. When physical capacity, congestion, or service rates are the real problems, changing prices won’t make a big difference in profits. This is consistent with the results here: the profit-based bandit increases margins modestly, but it cannot overcome the structural service limits set by oven capacity and queue dynamics.\nClassical inventory theory explains why the heuristic Newsvendor baseline works well here in a more general way. Arrow, Harris, and Marschak’s foundational work (Arrow, Harris, and Marschak 1951) and later reviews like Khouja’s (Khouja 1999) argue that intentional overproduction can be optimal in high-variance, single-period, or short-horizon systems because it protects against stockouts. This is exactly the behavior that leads to higher profits for the Newsvendor baseline in this environment.\nLastly, RL theory explains why PPO and QRDQN end up with conservative, low-production policies. García and Fernández (Garcı́a and Fernández 2015) found that reward structures with a lot of penalties make agents less likely to take risks. They also found that asymmetric penalties push policies toward minimizing exposure to uncertain or costly outcomes. Ng, Harada, and Russell (Ng, Harada, and Russell 1999) also show that changing the optimal policy by using non-potential-based reward shaping often leads to systematic biases. Because the shaped reward in this setting is not based on potential, this kind of distortion is to be expected. These pieces of literature all point to the same main point: the RL agents are acting rationally in relation to their training goal, and the reward—not the algorithm—is what really drives policy behavior in Bellman’s Bakery."
  },
  {
    "objectID": "analysis.html#overall-interpretation",
    "href": "analysis.html#overall-interpretation",
    "title": "Analysis & Discussion",
    "section": "Overall Interpretation",
    "text": "Overall Interpretation\nOutcomes reflect incentives and constraints, not algorithm failure. With a reward that overweights waste/delay and a binding service capacity, PPO and QRDQN rationally converge to low‑production policies; pricing and par‑levels do little against a capacity bottleneck, while Newsvendor’s overproduction prior fits this queueing regime. The remedy is not a different RL algorithm but a better‑aligned reward and/or capacity‑aware constraints; absent that, agents will continue to optimize away profit."
  },
  {
    "objectID": "appendix.html",
    "href": "appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "PPO: n_steps=2048, batch_size=256, gamma=0.995, gae_lambda=0.95, ent_coef=0.01.\nBandit arms: {0.9,1.0,1.1} (penalty metric), {0.7,0.85,1.0,1.15,1.3} (profit metric)."
  },
  {
    "objectID": "appendix.html#a.-hyperparameters",
    "href": "appendix.html#a.-hyperparameters",
    "title": "Appendix",
    "section": "",
    "text": "PPO: n_steps=2048, batch_size=256, gamma=0.995, gae_lambda=0.95, ent_coef=0.01.\nBandit arms: {0.9,1.0,1.1} (penalty metric), {0.7,0.85,1.0,1.15,1.3} (profit metric)."
  },
  {
    "objectID": "appendix.html#b.-environment-configurations",
    "href": "appendix.html#b.-environment-configurations",
    "title": "Appendix",
    "section": "B. Environment Configurations",
    "text": "B. Environment Configurations\n\nserve_per_tick=3; 2 ovens, capacity 4u each; patience 30–90s; queue cap 12; non‑stationarity on.\nPrices, costs, yields, sizes as specified in code constants."
  },
  {
    "objectID": "appendix.html#c.-additional-figures",
    "href": "appendix.html#c.-additional-figures",
    "title": "Appendix",
    "section": "C. Additional Figures",
    "text": "C. Additional Figures\n\nSee reports/figs/* for exported TensorBoard curves per run."
  },
  {
    "objectID": "environment.html",
    "href": "environment.html",
    "title": "Environment",
    "section": "",
    "text": "The Bellman’s Bakery environment models a capacity-constrained, perishable-goods service system with stochastic arrivals, queue dynamics, limited oven capacity, and nonstationary demand shifts. Each episode simulates a single operating day of 240 decision ticks (10 seconds each). At each tick the agent selects a discrete action—serving, baking, or idling—subject to inventory, queue state, and oven capacity.\nThe environment exposes operational trade-offs among production timing, waste, congestion, abandonment, and responsiveness to shifting demand."
  },
  {
    "objectID": "environment.html#viewer-snapshot",
    "href": "environment.html#viewer-snapshot",
    "title": "Environment",
    "section": "Viewer Snapshot",
    "text": "Viewer Snapshot"
  },
  {
    "objectID": "environment.html#state-actions-and-dynamics",
    "href": "environment.html#state-actions-and-dynamics",
    "title": "Environment",
    "section": "State, Actions, and Dynamics",
    "text": "State, Actions, and Dynamics\n\nState Representation\nLet \\(s_t\\) denote the state at tick \\(t\\).\nThe observation vector includes:\n\nInventory counts for all 5 items\n\nOven states: remaining-time fractions for 2 ovens\n\nQueue features:\n\nqueue length (cap 12)\n\ntop \\(K = 5\\) customers, each with\n\ndesired-item one-hot (5)\n\nremaining-patience fraction\n\n\n\nTime-of-day: sine and cosine encodings\n\nDaily price multiplier\n\nNonstationary context: ±10% daily drift, ±10% weekly item swings\n\n\n\nAction Space\nThere are 11 discrete actions, with masking of infeasible choices:\n1–5. Serve item \\(i\\)\n6–10. Bake item \\(i\\)\n11. Idle\nUp to 3 customers may be served per tick (serve_per_tick = 3).\nMasking applies \\(-\\infty\\) to logits of invalid actions.\nWhen any serve action is valid, bake actions are masked out to prioritize service.\n\n\nTransition Dynamics\n\nHorizon: 240 ticks\n\nArrivals: Poisson with morning and lunch peaks\n\nQueue cap: 12; overflow customers balk\n\nPatience: 30–90 s; expired customers abandon\n\nProduction: item-specific sizes, bake times, and yields\n\nEnd-of-day waste: leftover items incur cost\n\nRestocks: disabled\n\nNonstationarity: ±10% daily drift, ±10% weekly item shifts"
  },
  {
    "objectID": "environment.html#menu-capacities-and-parameters",
    "href": "environment.html#menu-capacities-and-parameters",
    "title": "Environment",
    "section": "Menu, Capacities, and Parameters",
    "text": "Menu, Capacities, and Parameters\n\nMenu Items\n\n\n\n\n\n\n\n\n\n\n\nItem\nSize (u)\nBake Time (s)\nBatch Yield\nBase Price\nCost (40%)\n\n\n\n\nMini Red Velvet Cake\n3\n90\n2\n$6.00\n$2.40\n\n\nRaspberry Matcha Roll\n1.5\n60\n4\n$4.50\n$1.80\n\n\nStrawberry Cream Slice\n1\n36\n4\n$3.50\n$1.40\n\n\nChocolate & Almond Drip Cake\n4\n120\n1\n$8.00\n$3.20\n\n\nChocolate Orange Roll\n1.5\n60\n4\n$4.50\n$1.80\n\n\n\n\n\nOvens & Initial Conditions\n\nOvens: 2\n\nCapacity: 4 units each\n\nStarting inventory: Strawberry=6, Matcha=4, Orange=4, Red Velvet=2, Drip Cake=1\n\n\n\nDemand & Pricing\n\nDemand mix: 30% Strawberry, 20% Matcha, 20% Orange, 15% Red Velvet, 15% Drip Cake\n\nAverage customers/day: ~60 (with morning and lunch peaks)\n\nDaily price multipliers: {0.9, 1.0, 1.1}\n\nPrice elasticity: none in this version — multipliers scale revenue only (demand unchanged)"
  },
  {
    "objectID": "environment.html#reward-function",
    "href": "environment.html#reward-function",
    "title": "Environment",
    "section": "Reward Function",
    "text": "Reward Function\nThe per-tick reward \\(r_t\\) is:\n\\[\nr_t = (\\text{price}_i - \\text{cost}_i)\\mathbf{1}\\{\\text{serve } i\\}\n+ 0.1 \\, \\mathbf{1}\\{\\text{serve } i\\}\n- 0.01 \\cdot \\text{queue\\_length}_t\n- 0.5 \\, \\mathbf{1}\\{\\text{abandon}\\}\n- 0.1 \\, \\mathbf{1}\\{\\text{balk}\\}.\n\\]\nAt episode end:\n\\[\nr_{\\text{end}} = -\\sum_i \\text{cost}_i \\cdot \\text{leftover}_i.\n\\]\nNo idle penalty is used.\n\nInterpretation\n\nRevenue and service events contribute positively.\n\nQueueing, abandonment, and balking incur penalties.\n\nEnd-of-day leftovers add cost.\n\nIllegal actions are softly penalized and treated as idle.\n\nBecause leftover cost equals full ingredient cost, the reward overweights waste avoidance, biasing the agent toward conservative baking."
  },
  {
    "objectID": "environment.html#business-objective-vs-rl-objective",
    "href": "environment.html#business-objective-vs-rl-objective",
    "title": "Environment",
    "section": "Business Objective vs RL Objective",
    "text": "Business Objective vs RL Objective\n\nBusiness KPI: maximize net profit with acceptable wait and abandonment levels.\n\nRL objective: maximize the shaped reward above.\n\nThe shaped reward trades off multiple goals and is not identical to maximizing net profit. This misalignment explains PPO’s conservative behavior despite appearing “irrational” from a profit perspective."
  },
  {
    "objectID": "environment.html#observationaction-interface",
    "href": "environment.html#observationaction-interface",
    "title": "Environment",
    "section": "Observation–Action Interface",
    "text": "Observation–Action Interface\nThe environment supplies:\n\nstructured vector observations\n\nan 11-action discrete space with masking\n\nfull MDP transitions\n\nSuitable for PPO, QRDQN, and entropy-regularized methods."
  },
  {
    "objectID": "environment.html#training-configuration",
    "href": "environment.html#training-configuration",
    "title": "Environment",
    "section": "Training Configuration",
    "text": "Training Configuration\n\nAlgorithm: PPO (Stable-Baselines3) with masking\n\nParallel envs: 8\n\nTraining budget: \\(3\\times10^5\\) → \\(10^6\\) steps\n\nEvaluation metrics: net profit, wait time, abandonment, waste rate, service level\n\nPPO provides stable learning in long-horizon, stochastic, discrete-action settings but cannot compensate for reward–metric misalignment."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "Operational choices in retail or service settings typically involve balancing multiple competing goals, such as increasing revenue, reducing customer wait times, preventing stockouts, and minimizing spoilage. Bakeries, for example, face short-term challenges with shelf-life items, limited space, and unpredictable demand that varies by hour. The well-known newsvendor model provides clear guidance for single-period ordering decisions, offering analytically optimal stocking levels under uncertain demand (Khouja 1999) (Arrow, Harris, and Marschak 1951). However, these formulations focus primarily on a single objective and do not account for real-time dynamics such as delays, customer abandonment, congestion, or operational bottlenecks.\nDeep reinforcement learning (DRL) has emerged as a useful method for developing control strategies through direct experience in complex settings. While these approaches can capture responses to shifting demand patterns, delays, or system limits, even without predefined models, they do not inherently aim to achieve goals like revenue or performance. DRL agents are only as good as the reward function they are trained on. They do not “optimize for profit” or “maximize customer satisfaction” unless those goals are explicitly and precisely defined. If the reward signal captures only part of what matters to a business, the resulting behavior can end up looking nothing like what a human decision-maker would actually want."
  },
  {
    "objectID": "introduction.html#motivation",
    "href": "introduction.html#motivation",
    "title": "Introduction",
    "section": "",
    "text": "Operational choices in retail or service settings typically involve balancing multiple competing goals, such as increasing revenue, reducing customer wait times, preventing stockouts, and minimizing spoilage. Bakeries, for example, face short-term challenges with shelf-life items, limited space, and unpredictable demand that varies by hour. The well-known newsvendor model provides clear guidance for single-period ordering decisions, offering analytically optimal stocking levels under uncertain demand (Khouja 1999) (Arrow, Harris, and Marschak 1951). However, these formulations focus primarily on a single objective and do not account for real-time dynamics such as delays, customer abandonment, congestion, or operational bottlenecks.\nDeep reinforcement learning (DRL) has emerged as a useful method for developing control strategies through direct experience in complex settings. While these approaches can capture responses to shifting demand patterns, delays, or system limits, even without predefined models, they do not inherently aim to achieve goals like revenue or performance. DRL agents are only as good as the reward function they are trained on. They do not “optimize for profit” or “maximize customer satisfaction” unless those goals are explicitly and precisely defined. If the reward signal captures only part of what matters to a business, the resulting behavior can end up looking nothing like what a human decision-maker would actually want."
  },
  {
    "objectID": "introduction.html#problem-formulation",
    "href": "introduction.html#problem-formulation",
    "title": "Introduction",
    "section": "Problem Formulation",
    "text": "Problem Formulation\nBellman’s Bakery is a custom OpenAI Gymnasium environment designed for this study to simulate a small bakery with capacity constraints, where each episode corresponds to one single day of operation. Customers arrive with stochastic demand and limited patience; they may leave at the sight of a long queue or if their wait times exceed a certain threshold. The agent(baker) decides which items to produce, when to serve customers, and when to idle, all while operating under constraints such as oven capacity and fixed bake times. At the end of each day, unsold inventory is discarded. The environment records business KPIs such as net profit, average wait, abandonment, and waste, and uses an action mask to enforce valid decisions. Its reward is a combined (weighted) signal: revenue and successful service add points, while waiting, abandonment, balking, and leftover inventory subtract points. This reward is intentionally not equivalent to net profit. The central question is how different solution approaches, newsvendor-styleheuristics, Proximal Policy Optimization (PPO), PPO+bandits, and Quantile Regression Deep Q‑Network (QRDQN), optimize this shaped reward and how the resulting policies perform when judged against business-facing metrics such as profitability, service quality, and inventory efficiency.\nTo investigate this, four types of policies are benchmarked:\n\nA newsvendor-style heuristic that selects bake quantities using a static demand model and a target service level (quantile rule).\nA PPO agent trained directly on the shaped reward in the Bellman’s Bakery environment.\nA hybrid PPO+bandit variant that augments PPO with a pricing or par-level bandit component.\nA QRDQN agent, representing an off-policy, value-based DRL method adapted to the same discrete action space.\n\nThis setup leads to the following research questions:\n\nHow does reward shaping in a multi-objective production system influence the qualitative behavior of deep RL agents?\nUnder what conditions does a DRL policy outperform, match, or underperform a simple newsvendor heuristic when evaluated on profit, waiting time, abandonment, and waste?\nDo off-policy distributional methods such as QRDQN exhibit different trade-offs than on-policy methods like PPO under the same reward and environment?"
  },
  {
    "objectID": "introduction.html#related-work",
    "href": "introduction.html#related-work",
    "title": "Introduction",
    "section": "Related Work",
    "text": "Related Work\nDespite substantial work at the intersection of perishable inventory, dynamic pricing, and reinforcement learning, most studies analyze settings that differ from the operational context examined in this paper. Research on perishable inventory typically focuses on optimal ordering and pricing strategies under stochastic demand, often assuming well-structured demand models and low-dimensional state spaces.For instance, Nomura and colleagues examined a perishable inventory issue involving dynamic pricing and demand that varies by age comparing traditional dynamic programming approaches to PPO methods (Nomura, Liu, and Nishi 2025). In the end, they found that DRL could get close to the best possible results while simulataneously cutting down on the time needed for computations. Two conclusions are especially relevant here: (i) PPO performs well when the reward is tightly aligned with the economic objective, and (ii) reward design strongly shapes learned behavior.\nOther lines of research examine RL in queueing and scheduling environments. Van Hezewijk shows that RL agents often prioritize surrogate reward terms, such as penalties for waiting or abandonment, over business-facing performance metrics when these are not tightly aligned (van Hezewijk 2024). This raises concerns about whether RL agents may develop overly conservative or low-throughput policies in systems with capacity constraints. In parallel, dynamic pricing research has often employed bandit-based approaches rather than full RL. Genalti demonstrates that Thompson sampling can effectively learn profit-maximizing pricing strategies under uncertainty, especially when demand elasticity is monotonic and data are limited (Genalti 2021). The pricing bandit used in this study reflects that lineage, though prior work suggests its effectiveness depends on whether pricing can meaningfully shift demand.\nTaken together, these works indicate that RL methods are highly sensitive to reward specification and state–action structure, and that bandit-based pricing only adds value when price elasticity meaningfully shifts demand. Accordingly, this paper investigates three core questions in the context of a simulated, capacity-constrained bakery: (1) how PPO responds to a shaped reward that only partially reflects profit; (2) whether surrogate penalties for waiting and abandonment bias policies toward conservative, low-waste behavior; and (3) whether pricing flexibility improves performance when demand is uncertain but throughput is constrained by service capacity rather than elasticity."
  },
  {
    "objectID": "introduction.html#contributions-of-this-project",
    "href": "introduction.html#contributions-of-this-project",
    "title": "Introduction",
    "section": "Contributions of this project",
    "text": "Contributions of this project\nThis project brings forward three key contributions. One involves a detailed queue-based simulation for a small bakery operation capturing those real tensions in daily work, like balancing profit against service quality and cutting down on waste. Another contribution comes from a structured comparison of various policy approaches through experiments that cover basic heuristics, on-policy reinforcement learning with PPO, hybrid bandit, RL methods, and an off-policy distributional technique called QRDQN. All of them faced evaluation under identical reward shaping. The third point reveals what happens when training rewards drift away from the actual evaluation measures. PPO starts to chase the wrong targets too hard. It pushes to minimize waste, even if that means sacrificing profit and service levels. Meanwhile, QRDQN along with heuristic policies land at varied spots on the same trade-off line. Overall, these outcomes point out something crucial: reward design and matching objectives really matter in deep reinforcement learning for production and inventory setups."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References\n\nArrow, Kenneth J, Theodore Harris, and Jacob Marschak. 1951. “Optimal Inventory Policy.” Econometrica 19 (3): 250–72. https://www.or.mist.i.u-tokyo.ac.jp/takeda/FreshmanCourse/ArrowHarrisMarschak.pdf.\n\n\nGarcı́a, Javier, and Fernando Fernández. 2015. “A Comprehensive Survey on Safe Reinforcement Learning.” Journal of Machine Learning Research 16 (1): 1437–80. https://jmlr.org/papers/volume16/garcia15a/garcia15a.pdf.\n\n\nGenalti, Gianmarco. 2021. “A Multi-Armed Bandit Approach to Dynamic Pricing.” Master's thesis, Laurea Magistrale in Mathematical Engineering - Ingegneria Matematica. https://www.politesi.polimi.it/bitstream/10589/183733/4/genalti_executive_summary.pdf.\n\n\nKhouja, Mohamed. 1999. “The Single-Period (Newsvendor) Problem: Literature Review and Suggestions for Future Research.” Omega 27 (5): 537–53. https://www-sciencedirect-com.proxy.library.georgetown.edu/science/article/pii/S0305048399000171.\n\n\nNg, Andrew Y, Daishi Harada, and Stuart Russell. 1999. “Policy Invariance Under Reward Transformations: Theory and Application to Reward Shaping.” In Proceedings of the Sixteenth International Conference on Machine Learning (ICML), 278–87. https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf.\n\n\nNomura, Yusuke, Ziang Liu, and Tatsushi Nishi. 2025. “Deep Reinforcement Learning for Dynamic Pricing and Ordering Policies in Perishable Inventory Management.” Applied Sciences 15 (5). https://doi.org/10.3390/app15052421.\n\n\nvan Hezewijk, Lotte. 2024. “Deep Reinforcement Learning for Production and Inventory Management: Bridging the Gap Between Theory and Practice.” Beta PhD Dissertation Series. Phd Thesis 1 (Research TU/e / Graduation TU/e), Industrial Engineering; Innovation Sciences; Eindhoven University of Technology."
  }
]