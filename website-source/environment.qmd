---
title: "Environment"
format:
  html:
    toc: true
    embed-resources: true
---

The Bellman’s Bakery environment models a capacity-constrained, perishable-goods service system with stochastic arrivals, queue dynamics, limited oven capacity, and nonstationary demand shifts. Each episode simulates a single operating day of **240 decision ticks** (10 seconds each). At each tick the agent selects a discrete action—serving, baking, or idling—subject to inventory, queue state, and oven capacity.

The environment exposes operational trade-offs among production timing, waste, congestion, abandonment, and responsiveness to shifting demand.

## Viewer Snapshot

![](../screenshots/bellman_bakery_viewer2.png){fig-cap="Bellman's Bakery Viewer"}

## State, Actions, and Dynamics

### State Representation

Let $s_t$ denote the state at tick $t$.  
The observation vector includes:

- **Inventory counts** for all 5 items  
- **Oven states:** remaining-time fractions for 2 ovens  
- **Queue features:**  
  - queue length (cap 12)  
  - top $K = 5$ customers, each with  
    - desired-item one-hot (5)  
    - remaining-patience fraction  
- **Time-of-day:** sine and cosine encodings  
- **Daily price multiplier**  
- **Nonstationary context:** ±10% daily drift, ±10% weekly item swings  

### Action Space

There are **11 discrete actions**, with masking of infeasible choices:

1–5. **Serve item $i$**  
6–10. **Bake item $i$**  
11. **Idle**

Up to **3 customers** may be served per tick (`serve_per_tick = 3`).  
Masking applies $-\infty$ to logits of invalid actions.  
When any serve action is valid, bake actions are masked out to prioritize service.

### Transition Dynamics

- **Horizon:** 240 ticks  
- **Arrivals:** Poisson with morning and lunch peaks  
- **Queue cap:** 12; overflow customers **balk**  
- **Patience:** 30–90 s; expired customers **abandon**  
- **Production:** item-specific sizes, bake times, and yields  
- **End-of-day waste:** leftover items incur cost  
- **Restocks:** disabled  
- **Nonstationarity:** ±10% daily drift, ±10% weekly item shifts  

## Menu, Capacities, and Parameters

### Menu Items

| Item | Size (u) | Bake Time (s) | Batch Yield | Base Price | Cost (40%) |
|------|----------|----------------|-------------|-------------|-------------|
| Mini Red Velvet Cake | 3 | 90 | 2 | \$6.00 | \$2.40 |
| Raspberry Matcha Roll | 1.5 | 60 | 4 | \$4.50 | \$1.80 |
| Strawberry Cream Slice | 1 | 36 | 4 | \$3.50 | \$1.40 |
| Chocolate & Almond Drip Cake | 4 | 120 | 1 | \$8.00 | \$3.20 |
| Chocolate Orange Roll | 1.5 | 60 | 4 | \$4.50 | \$1.80 |

### Ovens & Initial Conditions

- **Ovens:** 2  
- **Capacity:** 4 units each  
- **Starting inventory:** Strawberry=6, Matcha=4, Orange=4, Red Velvet=2, Drip Cake=1  

### Demand & Pricing

- **Demand mix:** 30% Strawberry, 20% Matcha, 20% Orange, 15% Red Velvet, 15% Drip Cake  
- **Average customers/day:** ~60 (with morning and lunch peaks)  
- **Daily price multipliers:** {0.9, 1.0, 1.1}  
- **Price elasticity:** none in this version — multipliers scale revenue only (demand unchanged)

## Reward Function

The per-tick reward $r_t$ is:

$$
r_t = (\text{price}_i - \text{cost}_i)\mathbf{1}\{\text{serve } i\}
+ 0.1 \, \mathbf{1}\{\text{serve } i\}
- 0.01 \cdot \text{queue\_length}_t
- 0.5 \, \mathbf{1}\{\text{abandon}\}
- 0.1 \, \mathbf{1}\{\text{balk}\}.
$$

At episode end:

$$
r_{\text{end}} = -\sum_i \text{cost}_i \cdot \text{leftover}_i.
$$

**No idle penalty** is used.

### Interpretation

- Revenue and service events contribute positively.  
- Queueing, abandonment, and balking incur penalties.  
- End-of-day leftovers add cost.  
- Illegal actions are softly penalized and treated as idle.

Because leftover cost equals full ingredient cost, the reward **overweights waste avoidance**, biasing the agent toward conservative baking.

## Business Objective vs RL Objective

- **Business KPI:** maximize net profit with acceptable wait and abandonment levels.  
- **RL objective:** maximize the shaped reward above.

The shaped reward trades off multiple goals and is **not identical** to maximizing net profit. This misalignment explains PPO’s conservative behavior despite appearing “irrational” from a profit perspective.

## Observation–Action Interface

The environment supplies:

- structured vector observations  
- an 11-action discrete space with masking  
- full MDP transitions  

Suitable for PPO, QRDQN, and entropy-regularized methods.

## Training Configuration

- **Algorithm:** PPO (Stable-Baselines3) with masking  
- **Parallel envs:** 8  
- **Training budget:** $3\times10^5$ → $10^6$ steps  
- **Evaluation metrics:** net profit, wait time, abandonment, waste rate, service level  

PPO provides stable learning in long-horizon, stochastic, discrete-action settings but cannot compensate for reward–metric misalignment.

